## End-to-End Machine Learning Project: Student Performance Indicator

Project Presentation : https://drive.google.com/file/d/1VlOd5lpA2uahUHYxHAKq37sOOwJlQ7WL/view?usp=sharing

### Project Overview

This project implements a comprehensive **end-to-end machine learning solution** focused on predicting student performance test scores. The aim is to understand how various factors such as gender, ethnicity, parental level of education, lunch, and test preparation courses affect student test scores. This project serves as a practical demonstration of **industry-standard best practices** in machine learning development, including **modular programming**, **object-oriented programming (OOP)**, and **Continuous Integration/Continuous Delivery (CI/CD)** pipeline implementation for deployment on **AWS Cloud**.

The dataset used consists of **1000 rows and 8 columns**, including both **categorical and numerical features**, and serves as a foundation for understanding complex data handling and model building.

### Key Features and Technologies

*   **Python Programming Language**: The core language for the entire application.
*   **Modular and Object-Oriented Design**: Ensures clean, maintainable, and reusable code components.
*   **Data Handling**: Utilises `pandas` for data manipulation and `numpy` for numerical operations.
*   **Machine Learning Algorithms**: Explores various regression algorithms (e.g., Linear Regression, Ridge, Lasso, Decision Tree, Random Forest, Gradient Boosting, AdaBoost, CatBoost, XGBoost) to identify the best performing model for student score prediction.
*   **Hyperparameter Tuning**: Incorporates techniques to optimise model performance.
*   **Feature Engineering & Data Preprocessing**: Handles missing values, performs one-hot encoding for categorical features, and standard scaling for numerical features.
*   **Logging and Exception Handling**: Customised robust logging and exception management for improved debugging and monitoring.
*   **Flask Web Application**: A lightweight web framework for creating the user interface and handling predictions.
*   **AWS Deployment**:
    *   **AWS Elastic Beanstalk**: Used for deploying the Flask web application, abstracting infrastructure management.
    *   **AWS CodePipeline**: Implements **Continuous Delivery (CD)**, automating the deployment process from GitHub to Elastic Beanstalk upon code changes.
*   **Git/GitHub**: Version control and collaborative code management, providing real-world industry experience.

### Project Structure (Modular Approach)

The project is structured with a strong emphasis on modularity and reusability, adhering to best practices for production-ready applications:

*   **Project Root Directory**:
    *   `.gitignore`: Specifies files and folders to be excluded from Git tracking (e.g., `venv`, `artifacts`).
    *   `setup.py`: Defines the project as an **installable Python package**, facilitating easy installation and dependency management. It uses `find_packages` to discover modules within `SRC`.
    *   `requirements.txt`: Lists all external Python packages required for the project. Includes `-e .` to trigger `setup.py` and install the project locally.
    *   `notebook` folder: Contains Jupyter notebooks (`Eda.ipynb`, `model_training.ipynb`) for initial **Exploratory Data Analysis (EDA)** and model experimentation, which are later converted into modular `.py` files.
    *   `app.py` / `application.py`: The main Flask web application entry point. `application.py` is a duplicate specifically used for AWS Elastic Beanstalk deployment.
    *   `templates` folder: Stores HTML files (`index.html`, `home.html`) for the web application's user interface.
    *   `.ebextensions` folder: Contains AWS Elastic Beanstalk deployment configuration, specifically `python.config`, which points to the WSGI path for the Flask application.
    *   `artifacts` folder: Centralised storage for all intermediate and final outputs of the ML pipeline, such as raw, train, and test datasets, preprocessor pickle files, and the best-trained model pickle file.
    *   `logs` folder: Stores log files generated by the custom logging system.

*   **`SRC` (Source) Folder**: The core of the project's logic, marked as a Python package with `__init__.py`.
    *   `components` folder: Houses independent modules of the machine learning lifecycle.
        *   `data_ingestion.py`: Responsible for **reading raw data** (initially from CSV, with future plans for MongoDB/API integration), performing **train-test splits**, and **saving datasets** to the `artifacts` folder.
        *   `data_transformation.py`: Handles **feature engineering** (e.g., one-hot encoding, standard scaling) and **data cleaning** (e.g., median/mode imputation for missing values). It saves a **`preprocessor.pickle` file**.
        *   `model_trainer.py`: Trains **multiple machine learning algorithms**, evaluates their performance, **selects the best model** (incorporating hyperparameter tuning), and **saves the best model as `model.pickle`**.
    *   `pipeline` folder: Defines end-to-end workflows.
        *   `train_pipeline.py` (Planned): Orchestrates the entire training process by sequentially calling `data_ingestion`, `data_transformation`, and `model_trainer`.
        *   `predict_pipeline.py`: Handles **predictions on new, unseen data** by loading the `preprocessor.pickle` and `model.pickle`, applying transformations, and using the model for inference. It includes a `CustomData` class to structure web application inputs.
    *   `logger.py`: Implements a **robust logging system** to track application execution and errors.
    *   `exception.py`: Defines a **custom exception handling mechanism** to capture detailed error information for debugging.
    *   `utils.py`: Contains **reusable utility functions** like `save_object` (for saving Python objects) and `load_object` (for loading pickle files), and `evaluate_model` (for assessing model performance).

### Data Flow and Outputs

The `artifacts` folder acts as a central repository for all processed data and trained models. The `data_ingestion` component saves `raw.csv`, `train.csv`, and `test.csv`. The `data_transformation` component saves `preprocessor.pickle`. Finally, the `model_trainer` saves the `model.pickle` of the best-performing model. These artifacts are then loaded by the `predict_pipeline` for making real-time predictions.

### Deployment Strategy

The application is deployed on **AWS Elastic Beanstalk** using a **Continuous Delivery (CD)** pipeline facilitated by **AWS CodePipeline**.
1.  **Application Setup**: The Flask application's entry point is configured for Elastic Beanstalk via `.ebextensions/python.config`.
2.  **CodePush to GitHub**: All code changes are committed and pushed to the GitHub repository.
3.  **CodePipeline Integration**: AWS CodePipeline is configured to monitor the GitHub repository for changes.
4.  **Automated Deployment**: Upon detecting new commits, CodePipeline automatically triggers a deployment to the configured Elastic Beanstalk environment, ensuring the application is always up-to-date.

### How to Run the Project (Locally)

1.  **Clone the Repository**:
    `git clone <repository_url>`
    `cd <project_directory>`
2.  **Create a Virtual Environment**:
    `conda create -p venv python=3.8 -y`
    `conda activate venv`
3.  **Install Dependencies**:
    `pip install -r requirements.txt`
4.  **Run the Application**:
    `python app.py` (or `python application.py` for deployment-like setup)
    Access the application via `http://127.0.0.1:5000/` in your browser.

---
